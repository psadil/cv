In my current position, my teammates and I have built a preprocessing and quality control pipeline for large medical imaging dataset. The data are being collected for a large, government funded project researching chronic pain and arrive from several hospitals in the US. Unfortunately, each of these sites provides the data in idiosyncratic formats, and so aggregating the data requires several stages of cleaning and harmonizing. I am responsible for configuring the quality control parts of the pipeline, to ensure that the incoming data conform to our expectations and will be usable for future analyses. To do this, I have configured automated analyses and generate reports that I present monthly to representatives from the hospitals. These reports have curtailed systematic issues in the data that would have, if unidentified, plagued the entire project. Instead, my reports have isolate the issues, providing actionable steps for the hospitals that would help improve the quality of the data they contribute.


Medical imaging, and specifically imaging of the brain, has, in the last few years, collected datasets that are large enough to support real-world applications of machine and deep learning, applications like providing predictions for susceptibility diseases such as dementia before the symptoms occur. This is exciting, and it opens new avenues for applying academic research in medical practices. And this nascence affords the chance to incorporate the lessons learned by other industries that have already transitioned into big data. For example, by now data scientists are well aware that biases can unintentionally be embedded into analyses and algorithms, biases that further systems of racism, sexism, ableism, or oppression broadly. To this end, I applied for and received a 50000 grant to research biases into proposed models for disease classification with medical images.

Early into the project, I realized a problem with the plan: although the available datasets were large, they were still highly unrepresentative of the general population. Specifically, although I had intended to assess biases in different ethnic groups, over 95% of the available data comprised individuals that were white. Clearly, the project would need to pivot, because I was not going to be able to say anything meaningful about relative accuracy across ethnicity. To reorient, I returned to the literature, to see what other critical questions about generalizability remained unknown but would be answerable with the data at hand.

As it turned out, basic questions about accuracy across assigned sex at birth are also unknown. For example, although sex is known to influence which chemicals are present in the brain, and although these chemicals can have substantial effects on the brain, many algorithms designed to predict diseases from images of the brain haven't been tested for differential accuracy between men and women. This was a lesson in keeping track of the broader goal. Rather than being too focused on the narrow task of assessing accuracy across ethnicity, I was able to push the project forward by remembering the broader goals of the analysis.

